{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "   - Simple Linear Regression is a statistical method that models the relationship between a dependent variable Y\n",
        " and a single independent variable X\n",
        " by fitting a linear equation Y =mX+cY=mX+c."
      ],
      "metadata": {
        "id": "Ri37NxK-VGRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What are the key assumptions of Simple Linear Regression?\n",
        "   - Linearity: Relationship between X and Yis linear.\n",
        "- Independence: Observations are independent.\n",
        "- Homoscedasticity: Constant variance of residuals/errors.\n",
        "- Normality: Residuals are normally distributed.\n",
        "- No multicollinearity (not applicable here since only one predictor)."
      ],
      "metadata": {
        "id": "Pd81uJcnWOjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "   - The coefficient m\n",
        " represents the slope of the regression line, indicating the change in\n",
        "Y\n",
        " for a one-unit change in\n",
        "X\n",
        "."
      ],
      "metadata": {
        "id": "a2PibYI2Wisi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "   - The intercept\n",
        "c\n",
        " is the predicted value of\n",
        "Y\n",
        " when\n",
        "X\n",
        "=\n",
        "0\n",
        "X=0\n",
        "."
      ],
      "metadata": {
        "id": "m2oI2nk2Wy_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "   - The slope\n",
        "m\n",
        " is calculated as\n",
        "- m\n",
        "=\n",
        "CCov(X,Y)/var(x)= ∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "ˉ\n",
        " )/∑(xi - x)^2\n",
        "\n"
      ],
      "metadata": {
        "id": "eXVymXePXCI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "   - The least squares method minimizes the sum of the squared differences between observed and predicted values, finding the best-fitting line."
      ],
      "metadata": {
        "id": "Tw0CpQZGYjXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "   - R^2 measures the proportion of variance in Y explained by X. value closer to 1 indicate a better fit."
      ],
      "metadata": {
        "id": "_IlunzF0Yr8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is Multiple Linear Regression?\n",
        "   - Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables."
      ],
      "metadata": {
        "id": "YvGqXhK_ZaiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "   - Simple Linear Regression has one predictor variable; Multiple Linear Regression has two or more predictors."
      ],
      "metadata": {
        "id": "HRwCoGfqZi4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "   - Linearity between predictors and outcome.\n",
        "- Independence of errors.\n",
        "- Homoscedasticity of residuals.\n",
        "- Normality of residuals.\n",
        "- No or low multicollinearity among predictors."
      ],
      "metadata": {
        "id": "wHo7Km0BZpfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "   - Heteroscedasticity means non-constant variance of residuals, which can lead to inefficient estimates and invalid standard errors, affecting hypothesis tests."
      ],
      "metadata": {
        "id": "POJ3JZBLZ5Lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "   - Remove or combine correlated variables.\n",
        "- Use dimensionality reduction (e.g., PCA).\n",
        "- Regularization techniques like Ridge or Lasso regression."
      ],
      "metadata": {
        "id": "-zOXLGsdaABY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "   - One-hot encoding (dummy variables).\n",
        "- Label encoding (for ordinal variables).\n",
        "- Effect coding."
      ],
      "metadata": {
        "id": "Yi7S3pUXaKwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "   - Interaction terms model the combined effect of two or more variables on the dependent variable, capturing non-additive relationships."
      ],
      "metadata": {
        "id": "-CsR3cfEaT36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "   - intercept is Y when X=0. In Multiple Regression, intercept is Y when all predictors are zero, which may be less meaningful if zero is outside the data range."
      ],
      "metadata": {
        "id": "3t6z9aiQabX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "   - The slope quantifies the expected change in Y per unit change in X, directly influencing prediction values."
      ],
      "metadata": {
        "id": "eacBn8qNa2v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  How does the intercept in a regression model provide context for the relationship between variables?\n",
        "   - It anchors the regression line, showing the baseline level of Y when predictors are zero."
      ],
      "metadata": {
        "id": "NKyMU6XwbGN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "   - Does not indicate causation.\n",
        "- Can increase with more predictors regardless of relevance.\n",
        "- Does not measure prediction accuracy on new data."
      ],
      "metadata": {
        "id": "_FkqXCgIbRuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "   - It indicates less precise estimation of the coefficient, suggesting uncertainty about the predictor's effect."
      ],
      "metadata": {
        "id": "VO7xdjGpbczK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "   -Residuals show a pattern or funnel shape instead of random scatter. Addressing it ensures valid inference and reliable confidence intervals."
      ],
      "metadata": {
        "id": "4HQPtR2wbj-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "   - It suggests that some predictors may not be meaningful; adjusted R^2 penalizes unnecessary variables."
      ],
      "metadata": {
        "id": "qE7BQLzTbs3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "   - Scaling helps when predictors have different units or magnitudes, improving numerical stability and interpretability, especially for regularization."
      ],
      "metadata": {
        "id": "yeazcgXTb6VN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  What is polynomial regression?\n",
        "   - Polynomial regression models the relationship between X and Y as an n-degree polynomial, allowing for nonlinear fits."
      ],
      "metadata": {
        "id": "MDQ4HcdEcBId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "   - Polynomial regression fits a curved line by including powers of X , while linear regression fits a straight line."
      ],
      "metadata": {
        "id": "VoVuQvdCcWsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "   - When the relationship between variables is nonlinear but can be approximated by a polynomial function"
      ],
      "metadata": {
        "id": "DLFGUIUxckus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "   - Y=β0+β1X+β2X2+β3X3+⋯+βnXn+ϵY"
      ],
      "metadata": {
        "id": "BjN0XOZXcriu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "   - Yes, by including polynomial terms and interactions of multiple predictors."
      ],
      "metadata": {
        "id": "absgs2mddLLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "   -  Can overfit with high-degree polynomials.\n",
        "- Sensitive to outliers.\n",
        "- Interpretation becomes complex."
      ],
      "metadata": {
        "id": "RV4wMuDPdRvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "   - ross-validation.\n",
        "- Adjusted R^2\n",
        "- AIC/BIC criteria.\n",
        "- Residual analysis"
      ],
      "metadata": {
        "id": "-Ab7PKEddl8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "   - It helps to understand the fit, detect overfitting, and communicate nonlinear relationships clearly."
      ],
      "metadata": {
        "id": "NXo07d6kd0ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "   - Using libraries like scikit-learn with PolynomialFeatures to transform input data, then fitting a linear regression model on the transformed features."
      ],
      "metadata": {
        "id": "kVJOyuuZd-Q0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "075f8be5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate some sample data for demonstration\n",
        "X = np.random.rand(100, 1) * 10  # 100 samples, 1 feature\n",
        "y = 2 * X.squeeze() + 1 + np.random.randn(100) # Linear relationship with some noise\n",
        "\n",
        "# Split data into training and testing sets (optional but good practice)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}